searchState.loadedDescShard("opencv", 3, "Ask layer if it support specific backend for doing …\nAsk layer if it support specific backend for doing …\nTarget identifier.\nTarget identifier.\nTarget identifier.\nTarget identifier.\nTarget identifier.\nC++ default parameters\nNote\nC++ default parameters\nNote\nImplement layers fusing.\nImplement layers fusing.\nImplement layers fusing.\nImplement layers fusing.\nImplement layers fusing.\nTry to fuse current layer with a next one\nTry to fuse current layer with a next one\nTry to fuse current layer with a next one\nTry to fuse current layer with a next one\nTry to fuse current layer with a next one\nTries to quantize the given layer and compute the …\nTries to quantize the given layer and compute the …\nTries to quantize the given layer and compute the …\nTries to quantize the given layer and compute the …\nTries to quantize the given layer and compute the …\nType name which was used for creating layer by layer …\nType name which was used for creating layer by layer …\nType name which was used for creating layer by layer …\nType name which was used for creating layer by layer …\nType name which was used for creating layer by layer …\nType name which was used for creating layer by layer …\nType name which was used for creating layer by layer …\nType name which was used for creating layer by layer …\nType name which was used for creating layer by layer …\nType name which was used for creating layer by layer …\nUnregisters registered layer with specified type name. …\n“Deattaches” all the layers, attached to particular …\n“Deattaches” all the layers, attached to particular …\n“Deattaches” all the layers, attached to particular …\n“Deattaches” all the layers, attached to particular …\n“Deattaches” all the layers, attached to particular …\nCreate a text representation for a binary network stored …\nIf set, the image is read in any possible color format.\nIf set, the image is read in any possible color format.\nIf set, return 16-bit/32-bit image when the input has the …\nIf set, return 16-bit/32-bit image when the input has the …\nIf set, always convert image to the 3 channel BGR color …\nIf set, always convert image to the 3 channel BGR color …\nIf set, always convert image to the single channel …\nIf set, always convert image to the single channel …\nIf set, do not rotate the image according to EXIF’s …\nIf set, do not rotate the image according to EXIF’s …\nIf set, use the gdal driver for loading the image.\nIf set, use the gdal driver for loading the image.\nIf set, always convert image to the 3 channel BGR color …\nIf set, always convert image to the 3 channel BGR color …\nIf set, always convert image to the 3 channel BGR color …\nIf set, always convert image to the 3 channel BGR color …\nIf set, always convert image to the 3 channel BGR color …\nIf set, always convert image to the 3 channel BGR color …\nIf set, always convert image to the single channel …\nIf set, always convert image to the single channel …\nIf set, always convert image to the single channel …\nIf set, always convert image to the single channel …\nIf set, always convert image to the single channel …\nIf set, always convert image to the single channel …\nIf set, return the loaded image as is (with alpha channel, …\nIf set, return the loaded image as is (with alpha channel, …\noverride EXR compression type (ZIP_COMPRESSION = 3 is …\noverride EXR compression type (ZIP_COMPRESSION = 3 is …\nlossy 4-by-4 pixel block compression, fixed compression …\nlossy 4-by-4 pixel block compression, fixed compression …\nlossy 4-by-4 pixel block compression, flat fields are …\nlossy 4-by-4 pixel block compression, flat fields are …\nlossy DCT based compression, in blocks of 32 scanlines. …\nlossy DCT based compression, in blocks of 32 scanlines. …\nlossy DCT based compression, in blocks of 256 scanlines. …\nlossy DCT based compression, in blocks of 256 scanlines. …\nno compression\nno compression\npiz-based wavelet compression\npiz-based wavelet compression\nlossy 24-bit float compression\nlossy 24-bit float compression\nrun length encoding\nrun length encoding\nzlib compression, in blocks of 16 scan lines\nzlib compression, in blocks of 16 scan lines\nzlib compression, one scan line at a time\nzlib compression, one scan line at a time\noverride EXR storage type (FLOAT (FP32) is default)\noverride EXR storage type (FLOAT (FP32) is default)\nstore as FP32 (default)\nstore as FP32 (default)\nstore as HALF (FP16)\nstore as HALF (FP16)\nFor JPEG2000, use to specify the target compression rate …\nFor JPEG2000, use to specify the target compression rate …\nSeparate chroma quality level, 0 - 100, default is 0 - don…\nSeparate chroma quality level, 0 - 100, default is 0 - don…\nSeparate luma quality level, 0 - 100, default is 0 - don’…\nSeparate luma quality level, 0 - 100, default is 0 - don’…\nEnable JPEG features, 0 or 1, default is False.\nEnable JPEG features, 0 or 1, default is False.\nEnable JPEG features, 0 or 1, default is False.\nEnable JPEG features, 0 or 1, default is False.\nFor JPEG, it can be a quality from 0 to 100 (the higher is …\nFor JPEG, it can be a quality from 0 to 100 (the higher is …\nJPEG restart interval, 0 - 65535, default is 0 - no …\nJPEG restart interval, 0 - 65535, default is 0 - no …\nFor PAM, sets the TUPLETYPE field to the corresponding …\nFor PAM, sets the TUPLETYPE field to the corresponding …\nBinary level PNG, 0 or 1, default is 0.\nBinary level PNG, 0 or 1, default is 0.\nFor PNG, it can be the compression level from 0 to 9. A …\nFor PNG, it can be the compression level from 0 to 9. A …\nOne of cv::ImwritePNGFlags, default is …\nOne of cv::ImwritePNGFlags, default is …\nUse this value for normal data.\nUse this value for normal data.\nUse this value for data produced by a filter (or …\nUse this value for data produced by a filter (or …\nUsing this value prevents the use of dynamic Huffman …\nUsing this value prevents the use of dynamic Huffman …\nUse this value to force Huffman encoding only (no string …\nUse this value to force Huffman encoding only (no string …\nUse this value to limit match distances to one (run-length …\nUse this value to limit match distances to one (run-length …\nFor PPM, PGM, or PBM, it can be a binary format flag, 0 or …\nFor PPM, PGM, or PBM, it can be a binary format flag, 0 or …\nFor TIFF, use to specify the image compression scheme. See …\nFor TIFF, use to specify the image compression scheme. See …\nFor TIFF, use to specify which DPI resolution unit to set; …\nFor TIFF, use to specify which DPI resolution unit to set; …\nFor TIFF, use to specify the X direction DPI\nFor TIFF, use to specify the X direction DPI\nFor TIFF, use to specify the Y direction DPI\nFor TIFF, use to specify the Y direction DPI\nFor WEBP, it can be a quality from 1 to 100 (the higher is …\nFor WEBP, it can be a quality from 1 to 100 (the higher is …\nImread flags\nImwrite flags\nImwrite PAM specific tupletype flags used to define the ‘…\nImwrite PNG specific flags used to tune the compression …\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns true if the specified image can be decoded by …\nReturns true if an image with the specified filename can …\nReturns the number of images inside the give file\nReturns the number of images inside the give file\nReads an image from a buffer in memory.\nReads an image from a buffer in memory.\nEncodes an image into a memory buffer.\nEncodes an image into a memory buffer.\nLoads an image from a file.\nLoads an image from a file.\nLoads a multi-page image from a file.\nLoads a multi-page image from a file.\nLoads a of images of a multi-page image from a file.\nLoads a of images of a multi-page image from a file.\nSaves an image to a specified file.\nSaves an image to a specified file.\nThis is an overloaded member function, provided for …\n@overload multi-image overload for bindings\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nthe threshold value inline formula is a weighted sum …\nthe threshold value inline formula is a weighted sum …\nthe threshold value inline formula is a mean of the inline …\nthe threshold value inline formula is a mean of the inline …\nadaptive threshold algorithm\nSame as CCL_GRANA. It is preferable to use the flag with …\nSame as CCL_GRANA. It is preferable to use the flag with …\nSpaghetti Bolelli2019 algorithm for 8-way connectivity, …\nSpaghetti Bolelli2019 algorithm for 8-way connectivity, …\nBBDT Grana2010 algorithm for 8-way connectivity, SAUF …\nBBDT Grana2010 algorithm for 8-way connectivity, SAUF …\nBBDT Grana2010 algorithm for 8-way connectivity, SAUF …\nBBDT Grana2010 algorithm for 8-way connectivity, SAUF …\nSame as CCL_WU. It is preferable to use the flag with the …\nSame as CCL_WU. It is preferable to use the flag with the …\nSame as CCL_BOLELLI. It is preferable to use the flag with …\nSame as CCL_BOLELLI. It is preferable to use the flag with …\nSAUF Wu2009 algorithm for 8-way connectivity, SAUF …\nSAUF Wu2009 algorithm for 8-way connectivity, SAUF …\nThe total area (in pixels) of the connected component\nThe total area (in pixels) of the connected component\nThe vertical size of the bounding box\nThe vertical size of the bounding box\nThe leftmost (x) coordinate which is the inclusive start …\nThe leftmost (x) coordinate which is the inclusive start …\nMax enumeration value. Used internally only for memory …\nMax enumeration value. Used internally only for memory …\nThe topmost (y) coordinate which is the inclusive start of …\nThe topmost (y) coordinate which is the inclusive start of …\nThe horizontal size of the bounding box\nThe horizontal size of the bounding box\nstores absolutely all the contour points. That is, any 2 …\nstores absolutely all the contour points. That is, any 2 …\ncompresses horizontal, vertical, and diagonal segments and …\ncompresses horizontal, vertical, and diagonal segments and …\napplies one of the flavors of the Teh-Chin chain …\napplies one of the flavors of the Teh-Chin chain …\napplies one of the flavors of the Teh-Chin chain …\napplies one of the flavors of the Teh-Chin chain …\nBase class for Contrast Limited Adaptive Histogram …\nMutable methods for crate::imgproc::CLAHE\nConstant methods for crate::imgproc::CLAHE\nautumn\nautumn\nbone\nbone\ncividis\ncividis\ncool\ncool\ndeepgreen\ndeepgreen\nhot\nhot\nHSV\nHSV\ninferno\ninferno\njet\njet\nmagma\nmagma\nocean\nocean\nparula\nparula\npink\npink\nplasma\nplasma\nrainbow\nrainbow\nspring\nspring\nsummer\nsummer\nturbo\nturbo\ntwilight\ntwilight\ntwilight shifted\ntwilight shifted\nviridis\nviridis\nwinter\nwinter\nconvert between RGB/BGR and BGR555 (16-bit images)\nconvert between RGB/BGR and BGR555 (16-bit images)\nconvert between RGB/BGR and BGR565 (16-bit images)\nconvert between RGB/BGR and BGR565 (16-bit images)\nadd alpha channel to RGB or BGR image\nadd alpha channel to RGB or BGR image\nconvert between RGB/BGR and grayscale, […\nconvert between RGB/BGR and grayscale, […\nconvert RGB/BGR to HLS (hue lightness saturation) with H …\nconvert RGB/BGR to HLS (hue lightness saturation) with H …\nconvert RGB/BGR to HLS (hue lightness saturation) with H …\nconvert RGB/BGR to HLS (hue lightness saturation) with H …\nconvert RGB/BGR to HSV (hue saturation value) with H range …\nconvert RGB/BGR to HSV (hue saturation value) with H range …\nconvert RGB/BGR to HSV (hue saturation value) with H range …\nconvert RGB/BGR to HSV (hue saturation value) with H range …\nconvert RGB/BGR to CIE Lab, [color_convert_rgb_lab] “…\nconvert RGB/BGR to CIE Lab, [color_convert_rgb_lab] “…\nconvert RGB/BGR to CIE Luv, [color_convert_rgb_luv] “…\nconvert RGB/BGR to CIE Luv, [color_convert_rgb_luv] “…\nconvert between RGB and BGR color spaces (with or without …\nconvert between RGB and BGR color spaces (with or without …\nconvert RGB/BGR to CIE XYZ, [color_convert_rgb_xyz] “…\nconvert RGB/BGR to CIE XYZ, [color_convert_rgb_xyz] “…\nconvert RGB/BGR to luma-chroma (aka YCC), […\nconvert RGB/BGR to luma-chroma (aka YCC), […\nconvert between RGB/BGR and YUV\nconvert between RGB/BGR and YUV\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nremove alpha channel from RGB or BGR image\nremove alpha channel from RGB or BGR image\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nDemosaicing\nDemosaicing\nDemosaicing with alpha channel\nDemosaicing with alpha channel\nEdge-Aware Demosaicing\nEdge-Aware Demosaicing\nDemosaicing using Variable Number of Gradients\nDemosaicing using Variable Number of Gradients\nDemosaicing\nDemosaicing\nDemosaicing\nDemosaicing with alpha channel\nEdge-Aware Demosaicing\nDemosaicing using Variable Number of Gradients\nDemosaicing\nDemosaicing\nDemosaicing with alpha channel\nDemosaicing with alpha channel\nEdge-Aware Demosaicing\nEdge-Aware Demosaicing\nDemosaicing using Variable Number of Gradients\nDemosaicing using Variable Number of Gradients\nDemosaicing\nDemosaicing\nDemosaicing\nDemosaicing with alpha channel\nEdge-Aware Demosaicing\nDemosaicing using Variable Number of Gradients\nDemosaicing\nDemosaicing\nDemosaicing with alpha channel\nDemosaicing with alpha channel\nEdge-Aware Demosaicing\nEdge-Aware Demosaicing\nDemosaicing using Variable Number of Gradients\nDemosaicing using Variable Number of Gradients\nDemosaicing\nDemosaicing\nDemosaicing\nDemosaicing with alpha channel\nEdge-Aware Demosaicing\nDemosaicing using Variable Number of Gradients\nDemosaicing\nDemosaicing\nDemosaicing with alpha channel\nDemosaicing with alpha channel\nEdge-Aware Demosaicing\nEdge-Aware Demosaicing\nDemosaicing using Variable Number of Gradients\nDemosaicing using Variable Number of Gradients\nDemosaicing\nDemosaicing\nDemosaicing\nDemosaicing with alpha channel\nEdge-Aware Demosaicing\nDemosaicing using Variable Number of Gradients\nDemosaicing with alpha channel\nDemosaicing with alpha channel\nconvert between grayscale and BGR555 (16-bit images)\nconvert between grayscale and BGR555 (16-bit images)\nconvert between grayscale to BGR565 (16-bit images)\nconvert between grayscale to BGR565 (16-bit images)\nbackward conversions HLS to RGB/BGR with H range 0..180 if …\nbackward conversions HLS to RGB/BGR with H range 0..180 if …\nbackward conversions HLS to RGB/BGR with H range 0..255 if …\nbackward conversions HLS to RGB/BGR with H range 0..255 if …\nbackward conversions HSV to RGB/BGR with H range 0..180 if …\nbackward conversions HSV to RGB/BGR with H range 0..180 if …\nbackward conversions HSV to RGB/BGR with H range 0..255 if …\nbackward conversions HSV to RGB/BGR with H range 0..255 if …\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nRGB to YUV 4:2:0 family\nalpha premultiplication\nalpha premultiplication\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:2 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nYUV 4:2:0 family to RGB\nalpha premultiplication\nalpha premultiplication\nblock formula\nblock formula\nblock formula\nblock formula\nblock formula\nblock formula\nthe color conversion codes\nGNU Octave/MATLAB equivalent colormaps\nconnected components algorithm\nconnected components statistics\nthe contour approximation algorithm\ndistance = max(|x1-x2|,|y1-y2|)\ndistance = max(|x1-x2|,|y1-y2|)\ndistance = c^2(|x|/c-log(1+|x|/c)), c = 1.3998\ndistance = c^2(|x|/c-log(1+|x|/c)), c = 1.3998\ndistance = |x|&lt;c ? x^2/2 : c(|x|-c/2), c=1.345\ndistance = |x|&lt;c ? x^2/2 : c(|x|-c/2), c=1.345\ndistance = |x1-x2| + |y1-y2|\ndistance = |x1-x2| + |y1-y2|\nL1-L2 metric: distance = 2(sqrt(1+x*x/2) - 1))\nL1-L2 metric: distance = 2(sqrt(1+x*x/2) - 1))\nthe simple euclidean distance\nthe simple euclidean distance\neach connected component of zeros in src (as well as all …\neach connected component of zeros in src (as well as all …\neach zero pixel (and all the non-zero pixels closest to …\neach zero pixel (and all the non-zero pixels closest to …\nmask=3\nmask=3\nmask=5\nmask=5\nUser defined distance\nUser defined distance\ndistance = c^2/2(1-exp(-(x/c)^2)), c = 2.9846\ndistance = c^2/2(1-exp(-(x/c)^2)), c = 2.9846\ndistanceTransform algorithm flags\nMask size for distance transform\nDistance types for Distance Transform and M-estimators\nIf set, the difference between the current pixel and seed …\nIf set, the difference between the current pixel and seed …\nIf set, the function does not change the image ( newVal is …\nIf set, the function does not change the image ( newVal is …\nnormal size serif font\nnormal size serif font\nsmaller version of FONT_HERSHEY_COMPLEX\nsmaller version of FONT_HERSHEY_COMPLEX\nnormal size sans-serif font (more complex than …\nnormal size sans-serif font (more complex than …\nsmall size sans-serif font\nsmall size sans-serif font\nmore complex variant of FONT_HERSHEY_SCRIPT_SIMPLEX\nmore complex variant of FONT_HERSHEY_SCRIPT_SIMPLEX\nhand-writing style font\nhand-writing style font\nnormal size sans-serif font\nnormal size sans-serif font\nnormal size serif font (more complex than …\nnormal size serif font (more complex than …\nflag for italic font\nflag for italic font\nfloodfill algorithm flags\nan obvious background pixels\nan obvious background pixels\nThe value means that the algorithm should just resume.\nThe value means that the algorithm should just resume.\nThe value means that the algorithm should just run the …\nThe value means that the algorithm should just run the …\nan obvious foreground (object) pixel\nan obvious foreground (object) pixel\nThe function initializes the state using the provided …\nThe function initializes the state using the provided …\nThe function initializes the state and the mask using the …\nThe function initializes the state and the mask using the …\na possible background pixel\na possible background pixel\na possible foreground pixel\na possible foreground pixel\nfinds arbitrary template in the grayscale image using …\nfinds arbitrary template in the grayscale image using …\nMutable methods for crate::imgproc::GeneralizedHoughBallard\nConstant methods for …\nfinds arbitrary template in the grayscale image using …\nMutable methods for crate::imgproc::GeneralizedHoughGuil\nConstant methods for crate::imgproc::GeneralizedHoughGuil\nMutable methods for crate::imgproc::GeneralizedHough\nConstant methods for crate::imgproc::GeneralizedHough\nclass of the pixel in GrabCut algorithm\nGrabCut algorithm flags\nBhattacharyya distance (In fact, OpenCV computes Hellinger …\nBhattacharyya distance (In fact, OpenCV computes Hellinger …\nChi-Square block formula\nChi-Square block formula\nAlternative Chi-Square block formula This alternative …\nAlternative Chi-Square block formula This alternative …\nCorrelation block formula where block formula and inline …\nCorrelation block formula where block formula and inline …\nSynonym for HISTCMP_BHATTACHARYYA\nIntersection block formula\nIntersection block formula\nKullback-Leibler divergence block formula\nKullback-Leibler divergence block formula\nbasically <em>21HT</em>, described in Yuen90\nbasically <em>21HT</em>, described in Yuen90\nvariation of HOUGH_GRADIENT to get better accuracy\nvariation of HOUGH_GRADIENT to get better accuracy\nmulti-scale variant of the classical Hough transform. The …\nmulti-scale variant of the classical Hough transform. The …\nprobabilistic Hough transform (more efficient in case if …\nprobabilistic Hough transform (more efficient in case if …\nclassical or standard Hough transform. Every line is …\nclassical or standard Hough transform. Every line is …\nOnly a subset of Hershey fonts …\nHistogram comparison methods @ingroup imgproc_hist\nVariants of a Hough transform\nOne of the rectangle is fully enclosed in the other\nOne of the rectangle is fully enclosed in the other\nNo intersection\nNo intersection\nThere is a partial intersection\nThere is a partial intersection\nresampling using pixel area relation. It may be a …\nresampling using pixel area relation. It may be a …\nbicubic interpolation\nbicubic interpolation\nLanczos interpolation over 8x8 neighborhood\nLanczos interpolation over 8x8 neighborhood\nbilinear interpolation\nbilinear interpolation\nBit exact bilinear interpolation\nBit exact bilinear interpolation\nmask for interpolation codes\nmask for interpolation codes\nnearest neighbor interpolation\nnearest neighbor interpolation\nBit exact nearest neighbor interpolation. This will …\nBit exact nearest neighbor interpolation. This will …\nIntelligent Scissors image segmentation\nMutable methods for crate::imgproc::IntelligentScissorsMB\nConstant methods for crate::imgproc::IntelligentScissorsMB\ninterpolation algorithm\n4-connected line\n4-connected line\n8-connected line\n8-connected line\nantialiased line\nantialiased line\nAdvanced refinement. Number of false alarms is calculated, …\nAdvanced refinement. Number of false alarms is calculated, …\nNo refinement applied\nNo refinement applied\nStandard refinement is applied. E.g. breaking arches into …\nStandard refinement is applied. E.g. breaking arches into …\nLine iterator\nMutable methods for crate::imgproc::LineIterator\nConstant methods for crate::imgproc::LineIterator\nLine segment detector class\nVariants of Line Segment %Detector\nMutable methods for crate::imgproc::LineSegmentDetector\nConstant methods for crate::imgproc::LineSegmentDetector\ntypes of line @ingroup imgproc_draw\nA crosshair marker shape\nA crosshair marker shape\nA diamond marker shape\nA diamond marker shape\nA square marker shape\nA square marker shape\nA star marker shape, combination of cross and tilted cross\nA star marker shape, combination of cross and tilted cross\nA 45 degree tilted crosshair marker shape\nA 45 degree tilted crosshair marker shape\nA downwards pointing triangle marker shape\nA downwards pointing triangle marker shape\nAn upwards pointing triangle marker shape\nAn upwards pointing triangle marker shape\n“black hat” block formula\n“black hat” block formula\na closing operation block formula\na closing operation block formula\na cross-shaped structuring element: block formula\na cross-shaped structuring element: block formula\nsee #dilate\nsee #dilate\nan elliptic structuring element, that is, a filled ellipse …\nan elliptic structuring element, that is, a filled ellipse …\nsee #erode\nsee #erode\na morphological gradient block formula\na morphological gradient block formula\n“hit or miss” .- Only supported for CV_8UC1 binary …\n“hit or miss” .- Only supported for CV_8UC1 binary …\nan opening operation block formula\nan opening operation block formula\na rectangular structuring element:  block formula\na rectangular structuring element:  block formula\n“top hat” block formula\n“top hat” block formula\nPossible set of marker types used for the cv::drawMarker …\nshape of the structuring element\ntype of morphological operation\nretrieves all of the contours and organizes them into a …\nretrieves all of the contours and organizes them into a …\nretrieves only the extreme outer contours. It sets …\nretrieves only the extreme outer contours. It sets …\nretrieves all of the contours without establishing any …\nretrieves all of the contours without establishing any …\nretrieves all of the contours and reconstructs a full …\nretrieves all of the contours and reconstructs a full …\ntypes of intersection between rectangles\nmode of the contour retrieval algorithm\nShape matching methods\nMutable methods for crate::imgproc::Subdiv2D\nConstant methods for crate::imgproc::Subdiv2D\nPoint location error\nPoint inside some facet\nPoint on some edge\nPoint outside the subdivision bounding rect\nPoint coincides with one of the subdivision vertices\nblock formula\nblock formula\nblock formula\nblock formula\nflag, use Otsu algorithm to choose the optimal threshold …\nflag, use Otsu algorithm to choose the optimal threshold …\nblock formula\nblock formula\nblock formula\nblock formula\nflag, use Triangle algorithm to choose the optimal …\nflag, use Triangle algorithm to choose the optimal …\nblock formula\nblock formula\n!&lt; block formula where block formula with mask: block …\n!&lt; block formula where block formula with mask: block …\n!&lt; block formula\n!&lt; block formula\n!&lt; block formula with mask: block formula\n!&lt; block formula with mask: block formula\n!&lt; block formula with mask: block formula\n!&lt; block formula with mask: block formula\n!&lt; block formula with mask: block formula\n!&lt; block formula with mask: block formula\n!&lt; block formula with mask: block formula\n!&lt; block formula with mask: block formula\ntype of the template matching operation\ntype of the threshold operation threshold types\nflag, fills all of the destination image pixels. If some …\nflag, fills all of the destination image pixels. If some …\nflag, inverse transformation\nflag, inverse transformation\nRemaps an image to/from polar space.\nRemaps an image to/from polar space.\nRemaps an image to/from semilog-polar space.\nRemaps an image to/from semilog-polar space.\n\\brief Specify the polar mapping mode\nAdds an image to the accumulator image.\nAdds an image to the accumulator image.\nAdds the per-element product of two input images to the …\nAdds the per-element product of two input images to the …\nAdds the square of a source image to the accumulator image.\nAdds the square of a source image to the accumulator image.\nUpdates a running average.\nUpdates a running average.\nApplies an adaptive threshold to an array.\nEqualizes the histogram of a grayscale image using …\nEqualizes the histogram of a grayscale image using …\nEqualizes the histogram of a grayscale image using …\nEqualizes the histogram of a grayscale image using …\nEqualizes the histogram of a grayscale image using …\nApplies a GNU Octave/MATLAB equivalent colormap on a given …\nApplies a user colormap on a given image.\nSpecify input image and extract image features\nSpecify input image and extract image features\nSpecify input image and extract image features\nSpecify input image and extract image features\nSpecify input image and extract image features\nSpecify custom features of imput image\nSpecify custom features of imput image\nSpecify custom features of imput image\nSpecify custom features of imput image\nSpecify custom features of imput image\nSpecify custom features of imput image\nSpecify custom features of imput image\nSpecify custom features of imput image\nSpecify custom features of imput image\nSpecify custom features of imput image\nApproximates a polygonal curve(s) with the specified …\nCalculates a contour perimeter or a curve length.\nDraws an arrow segment pointing from the first point to …\nDraws an arrow segment pointing from the first point to …\nApplies the bilateral filter to an image.\nApplies the bilateral filter to an image.\nPerforms linear blending of two images: block formula\nBlurs an image using the normalized box filter.\nBlurs an image using the normalized box filter.\nCalculates the up-right bounding rectangle of a point set …\nBlurs an image using the box filter.\nBlurs an image using the box filter.\nFinds the four vertices of a rotated rect. Useful to draw …\nPrepares a map of optimal paths for the given source point …\nPrepares a map of optimal paths for the given source point …\nPrepares a map of optimal paths for the given source point …\nPrepares a map of optimal paths for the given source point …\nPrepares a map of optimal paths for the given source point …\nConstructs the Gaussian pyramid for an image.\nConstructs the Gaussian pyramid for an image.\nCalculates the back projection of a histogram.\nCalculates a histogram of a set of arrays.\n@overload\nFinds edges in an image using the Canny algorithm Canny86 .\nFinds edges in an image using the Canny algorithm Canny86 .\n\\overload\n\\overload\nDraws a circle.\nDraws a circle.\nClips the line against the image rectangle.\nClips the line against the image rectangle.\nClips the line against the image rectangle.\nCompares two histograms.\nCompares two histograms.\nDraws two groups of lines in blue and red, counting the …\nDraws two groups of lines in blue and red, counting the …\nDraws two groups of lines in blue and red, counting the …\nDraws two groups of lines in blue and red, counting the …\nDraws two groups of lines in blue and red, counting the …\nDraws two groups of lines in blue and red, counting the …\nDraws two groups of lines in blue and red, counting the …\nDraws two groups of lines in blue and red, counting the …\nDraws two groups of lines in blue and red, counting the …\nDraws two groups of lines in blue and red, counting the …\ncomputes the connected components labeled image of boolean …\n@overload\ncomputes the connected components labeled image of boolean …\ncomputes the connected components labeled image of boolean …\n@overload\ncomputes the connected components labeled image of boolean …\nCalculates a contour area.\nCalculates a contour area.\nConverts image transformation maps from one representation …\nConverts image transformation maps from one representation …\nFinds the convex hull of a point set.\nFinds the convex hull of a point set.\nFinds the convexity defects of a contour.\nCalculates eigenvalues and eigenvectors of image blocks …\nCalculates eigenvalues and eigenvectors of image blocks …\nHarris corner detector.\nHarris corner detector.\nCalculates the minimal eigenvalue of gradient matrices for …\nCalculates the minimal eigenvalue of gradient matrices for …\nRefines the corner locations.\nCreates a smart pointer to a cv::CLAHE class and …\nCreates a smart pointer to a cv::CLAHE class and …\nCreates a smart pointer to a cv::GeneralizedHoughBallard …\nCreates a smart pointer to a cv::GeneralizedHoughGuil …\nThis function computes a Hanning window coefficients in …\nCreates a smart pointer to a LineSegmentDetector object …\nCreates a smart pointer to a LineSegmentDetector object …\nConverts an image from one color space to another.\nConverts an image from one color space to another.\nConverts an image from one color space to another where …\ncreates an empty Subdiv2D object. To create a new empty …\nmain function for all demosaicing processes\nmain function for all demosaicing processes\nfind template on image\nfind template on image\nfind template on image\nfind template on image\nfind template on image\nFinds lines in the input image.\nFinds lines in the input image.\nFinds lines in the input image.\nFinds lines in the input image.\nFinds lines in the input image.\nfind template on image\nfind template on image\nfind template on image\nfind template on image\nfind template on image\nFinds lines in the input image.\nFinds lines in the input image.\nFinds lines in the input image.\nFinds lines in the input image.\nFinds lines in the input image.\nC++ default parameters\nC++ default parameters\nC++ default parameters\nC++ default parameters\nC++ default parameters\nNote\nNote\nNote\nNote\nNote\nDilates an image by using a specific structuring element.\nDilates an image by using a specific structuring element.\nCalculates the distance to the closest zero pixel for each …\n@overload\nCalculates the distance to the closest zero pixel for each …\nCalculates the distance to the closest zero pixel for each …\nPerforms the per-element division of the first Fourier …\nPerforms the per-element division of the first Fourier …\nDraws contours outlines or filled contours.\nDraws contours outlines or filled contours.\nDraws a marker on a predefined position in an image.\nDraws a marker on a predefined position in an image.\nDraws the line segments on a given image.\nDraws the line segments on a given image.\nDraws the line segments on a given image.\nDraws the line segments on a given image.\nDraws the line segments on a given image.\nReturns the edge destination.\nReturns the edge destination.\nReturns the edge destination.\nReturns the edge destination.\nReturns the edge destination.\nReturns the edge destination.\nReturns the edge destination.\nReturns the edge destination.\nReturns the edge destination.\nReturns the edge destination.\nReturns the edge origin.\nReturns the edge origin.\nReturns the edge origin.\nReturns the edge origin.\nReturns the edge origin.\nReturns the edge origin.\nReturns the edge origin.\nReturns the edge origin.\nReturns the edge origin.\nReturns the edge origin.\nDraws a simple or thick elliptic arc or fills an ellipse …\nApproximates an elliptic arc with a polyline.\nApproximates an elliptic arc with a polyline.\nDraws a simple or thick elliptic arc or fills an ellipse …\nDraws a simple or thick elliptic arc or fills an ellipse …\n@overload\nComputes the “minimal work” distance between two …\nC++ default parameters\nNote\nComputes the “minimal work” distance between two …\nEqualizes the histogram of a grayscale image.\nErodes an image by using a specific structuring element.\nErodes an image by using a specific structuring element.\nFills a convex polygon.\nFills a convex polygon.\nFills the area bounded by one or more polygons.\nFills the area bounded by one or more polygons.\nConvolves an image with the kernel.\nConvolves an image with the kernel.\nFinds contours in a binary image.\n@overload\nFinds contours in a binary image.\nFinds contours in a binary image.\nFinds the subdivision vertex closest to the given point.\nFinds the subdivision vertex closest to the given point.\nFinds the subdivision vertex closest to the given point.\nFinds the subdivision vertex closest to the given point.\nFinds the subdivision vertex closest to the given point.\nFinds the subdivision vertex closest to the given point.\nFinds the subdivision vertex closest to the given point.\nFinds the subdivision vertex closest to the given point.\nFinds the subdivision vertex closest to the given point.\nFinds the subdivision vertex closest to the given point.\nFits an ellipse around a set of 2D points.\nFits an ellipse around a set of 2D points.\nFits an ellipse around a set of 2D points.\nFits a line to a 2D or 3D point set.\nFills a connected component with the given color.\n@overload\nFills a connected component with the given color.\nFills a connected component with the given color.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nBlurs an image using a Gaussian filter.\nBlurs an image using a Gaussian filter.\nCalculates an affine transform from three pairs of the …\nReturns threshold value for contrast limiting.\nReturns threshold value for contrast limiting.\nReturns threshold value for contrast limiting.\nReturns threshold value for contrast limiting.\nReturns threshold value for contrast limiting.\nExtracts optimal contour for the given target point on the …\nExtracts optimal contour for the given target point on the …\nExtracts optimal contour for the given target point on the …\nExtracts optimal contour for the given target point on the …\nExtracts optimal contour for the given target point on the …\nExtracts optimal contour for the given target point on the …\nExtracts optimal contour for the given target point on the …\nExtracts optimal contour for the given target point on the …\nExtracts optimal contour for the given target point on the …\nExtracts optimal contour for the given target point on the …\nReturns filter coefficients for computing spatial image …\nReturns filter coefficients for computing spatial image …\nReturns one of the edges related to the given edge.\nReturns one of the edges related to the given edge.\nReturns one of the edges related to the given edge.\nReturns one of the edges related to the given edge.\nReturns one of the edges related to the given edge.\nReturns a list of all edges.\nReturns a list of all edges.\nReturns a list of all edges.\nReturns a list of all edges.\nReturns a list of all edges.\nCalculates the font-specific size to use to achieve a …\nCalculates the font-specific size to use to achieve a …\nReturns Gabor filter coefficients.\nReturns Gabor filter coefficients.\nReturns Gaussian filter coefficients.\nReturns Gaussian filter coefficients.\nReturns a list of the leading edge ID connected to each …\nReturns a list of the leading edge ID connected to each …\nReturns a list of the leading edge ID connected to each …\nReturns a list of the leading edge ID connected to each …\nReturns a list of the leading edge ID connected to each …\nCalculates a perspective transform from four pairs of the …\nCalculates a perspective transform from four pairs of the …\nCalculates a perspective transform from four pairs of the …\n@overload\nRetrieves a pixel rectangle from an image with sub-pixel …\nRetrieves a pixel rectangle from an image with sub-pixel …\nCalculates an affine matrix of 2D rotation.\nSee also\nReturns a structuring element of the specified size and …\nReturns a structuring element of the specified size and …\nCalculates the width and height of a text string.\nReturns Size defines the number of tiles in row and column.\nReturns Size defines the number of tiles in row and column.\nReturns Size defines the number of tiles in row and column.\nReturns Size defines the number of tiles in row and column.\nReturns Size defines the number of tiles in row and column.\nReturns a list of all triangles.\nReturns a list of all triangles.\nReturns a list of all triangles.\nReturns a list of all triangles.\nReturns a list of all triangles.\nReturns vertex location from vertex ID.\nReturns vertex location from vertex ID.\nReturns vertex location from vertex ID.\nReturns vertex location from vertex ID.\nReturns vertex location from vertex ID.\nReturns vertex location from vertex ID.\nReturns vertex location from vertex ID.\nReturns vertex location from vertex ID.\nReturns vertex location from vertex ID.\nReturns vertex location from vertex ID.\nReturns a list of all Voronoi facets.\nReturns a list of all Voronoi facets.\nReturns a list of all Voronoi facets.\nReturns a list of all Voronoi facets.\nReturns a list of all Voronoi facets.\nDetermines strong corners on an image.\nDetermines strong corners on an image.\nC++ default parameters\nNote\nSame as above, but returns also quality measure of the …\nSame as above, but returns also quality measure of the …\nRuns the GrabCut algorithm.\nRuns the GrabCut algorithm.\nFinds circles in a grayscale image using the Hough …\nFinds circles in a grayscale image using the Hough …\nFinds lines in a binary image using the standard Hough …\nFinds lines in a binary image using the standard Hough …\nFinds line segments in a binary image using the …\nFinds line segments in a binary image using the …\nFinds lines in a set of points using the standard Hough …\nCalculates seven Hu invariants.\nCalculates seven Hu invariants.\nprefix increment operator (++it). shifts iterator to the …\nprefix increment operator (++it). shifts iterator to the …\nprefix increment operator (++it). shifts iterator to the …\nprefix increment operator (++it). shifts iterator to the …\nprefix increment operator (++it). shifts iterator to the …\nCreates a new empty Delaunay subdivision\nCreates a new empty Delaunay subdivision\nCreates a new empty Delaunay subdivision\nCreates a new empty Delaunay subdivision\nCreates a new empty Delaunay subdivision\nInsert a single point into a Delaunay triangulation.\nInsert a single point into a Delaunay triangulation.\nInsert a single point into a Delaunay triangulation.\nInsert a single point into a Delaunay triangulation.\nInsert a single point into a Delaunay triangulation.\nInsert multiple points into a Delaunay triangulation.\nInsert multiple points into a Delaunay triangulation.\nInsert multiple points into a Delaunay triangulation.\nInsert multiple points into a Delaunay triangulation.\nInsert multiple points into a Delaunay triangulation.\nCalculates the integral of an image.\nCalculates the integral of an image.\n@overload\nCalculates the integral of an image.\nCalculates the integral of an image.\n@overload\nFinds intersection of two convex polygons\nFinds intersection of two convex polygons\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nInverts an affine transformation.\nTests a contour convexity.\nCalculates the Laplacian of an image.\nCalculates the Laplacian of an image.\nDraws a line segment connecting two points.\nDraws a line segment connecting two points.\nRemaps an image to polar coordinates space.\nReturns the location of a point within a Delaunay …\nReturns the location of a point within a Delaunay …\nReturns the location of a point within a Delaunay …\nReturns the location of a point within a Delaunay …\nReturns the location of a point within a Delaunay …\nRemaps an image to semilog-polar coordinates space.\nCompares two shapes.\nCompares a template against overlapped image regions.\nCompares a template against overlapped image regions.\nBlurs an image using the median filter.\nFinds a rotated rectangle of the minimum area enclosing …\nFinds a circle of the minimum area enclosing a 2D point …\nFinds a triangle of minimum area enclosing a 2D point set …\nCalculates all of the moments up to the third order of a …\nCalculates all of the moments up to the third order of a …\nreturns “magic” border value for erosion and dilation. …\nPerforms advanced morphological transformations.\nPerforms advanced morphological transformations.\ninitializes the iterator\ncreates an empty Subdiv2D object. To create a new empty …\nC++ default parameters\nC++ default parameters\nC++ default parameters\ninitializes the iterator\nNote\nNote\nNote\nReturns next edge around the edge origin.\nReturns next edge around the edge origin.\nReturns next edge around the edge origin.\nReturns next edge around the edge origin.\nReturns next edge around the edge origin.\nThe function is used to detect translational shifts that …\nThe function is used to detect translational shifts that …\nPerforms a point-in-contour test.\nDraws several polygonal curves.\nDraws several polygonal curves.\nreturns coordinates of the current pixel\nreturns coordinates of the current pixel\nreturns coordinates of the current pixel\nreturns coordinates of the current pixel\nreturns coordinates of the current pixel\nCalculates a feature map for corner detection.\nCalculates a feature map for corner detection.\nDraws a text string.\nDraws a text string.\nBlurs an image and downsamples it.\nBlurs an image and downsamples it.\nPerforms initial step of meanshift segmentation of an …\nPerforms initial step of meanshift segmentation of an …\nUpsamples an image and then blurs it.\nUpsamples an image and then blurs it.\nDraws a simple, thick, or filled up-right rectangle.\n@overload\nDraws a simple, thick, or filled up-right rectangle.\nDraws a simple, thick, or filled up-right rectangle.\nApplies a generic geometrical transformation to an image.\nApplies a generic geometrical transformation to an image.\nResizes an image.\nResizes an image.\nReturns another edge of the same quad-edge.\nReturns another edge of the same quad-edge.\nReturns another edge of the same quad-edge.\nReturns another edge of the same quad-edge.\nReturns another edge of the same quad-edge.\nFinds out if there is any intersection between two rotated …\nCalculates the first x- or y- image derivative using …\nCalculates the first x- or y- image derivative using …\nApplies a separable linear filter to an image.\nApplies a separable linear filter to an image.\nMaximal difference between angles that treated as equal.\nMaximal difference between angles that treated as equal.\nMaximal difference between angles that treated as equal.\nMaximal difference between angles that treated as equal.\nMaximal difference between angles that treated as equal.\nAngle step in degrees.\nAngle step in degrees.\nAngle step in degrees.\nAngle step in degrees.\nAngle step in degrees.\nAngle votes threshold.\nAngle votes threshold.\nAngle votes threshold.\nAngle votes threshold.\nAngle votes threshold.\nCanny high threshold.\nCanny high threshold.\nCanny high threshold.\nCanny high threshold.\nCanny high threshold.\nCanny low threshold.\nCanny low threshold.\nCanny low threshold.\nCanny low threshold.\nCanny low threshold.\nSets threshold for contrast limiting.\nSets threshold for contrast limiting.\nSets threshold for contrast limiting.\nSets threshold for contrast limiting.\nSets threshold for contrast limiting.\nInverse ratio of the accumulator resolution to the image …\nInverse ratio of the accumulator resolution to the image …\nInverse ratio of the accumulator resolution to the image …\nInverse ratio of the accumulator resolution to the image …\nInverse ratio of the accumulator resolution to the image …\nSwitch edge feature extractor to use Canny edge detector\nSwitch edge feature extractor to use Canny edge detector\nSwitch edge feature extractor to use Canny edge detector\nSwitch edge feature extractor to use Canny edge detector\nSwitch edge feature extractor to use Canny edge detector\nSwitch edge feature extractor to use Canny edge detector\nSwitch edge feature extractor to use Canny edge detector\nSwitch edge feature extractor to use Canny edge detector\nSwitch edge feature extractor to use Canny edge detector\nSwitch edge feature extractor to use Canny edge detector\nSwitch to “Laplacian Zero-Crossing” edge feature …\nSwitch to “Laplacian Zero-Crossing” edge feature …\nSwitch to “Laplacian Zero-Crossing” edge feature …\nSwitch to “Laplacian Zero-Crossing” edge feature …\nSwitch to “Laplacian Zero-Crossing” edge feature …\nSwitch to “Laplacian Zero-Crossing” edge feature …\nSwitch to “Laplacian Zero-Crossing” edge feature …\nSwitch to “Laplacian Zero-Crossing” edge feature …\nSwitch to “Laplacian Zero-Crossing” edge feature …\nSwitch to “Laplacian Zero-Crossing” edge feature …\nSpecify gradient magnitude max value threshold\nSpecify gradient magnitude max value threshold\nSpecify gradient magnitude max value threshold\nSpecify gradient magnitude max value threshold\nSpecify gradient magnitude max value threshold\nSpecify gradient magnitude max value threshold\nSpecify gradient magnitude max value threshold\nSpecify gradient magnitude max value threshold\nSpecify gradient magnitude max value threshold\nSpecify gradient magnitude max value threshold\nR-Table levels.\nR-Table levels.\nR-Table levels.\nR-Table levels.\nR-Table levels.\nFeature table levels.\nFeature table levels.\nFeature table levels.\nFeature table levels.\nFeature table levels.\nMaximal rotation angle to detect in degrees.\nMaximal rotation angle to detect in degrees.\nMaximal rotation angle to detect in degrees.\nMaximal rotation angle to detect in degrees.\nMaximal rotation angle to detect in degrees.\nMaximal size of inner buffers.\nMaximal size of inner buffers.\nMaximal size of inner buffers.\nMaximal size of inner buffers.\nMaximal size of inner buffers.\nMaximal scale to detect.\nMaximal scale to detect.\nMaximal scale to detect.\nMaximal scale to detect.\nMaximal scale to detect.\nMinimal rotation angle to detect in degrees.\nMinimal rotation angle to detect in degrees.\nMinimal rotation angle to detect in degrees.\nMinimal rotation angle to detect in degrees.\nMinimal rotation angle to detect in degrees.\nMinimum distance between the centers of the detected …\nMinimum distance between the centers of the detected …\nMinimum distance between the centers of the detected …\nMinimum distance between the centers of the detected …\nMinimum distance between the centers of the detected …\nMinimal scale to detect.\nMinimal scale to detect.\nMinimal scale to detect.\nMinimal scale to detect.\nMinimal scale to detect.\nPosition votes threshold.\nPosition votes threshold.\nPosition votes threshold.\nPosition votes threshold.\nPosition votes threshold.\nScale step.\nScale step.\nScale step.\nScale step.\nScale step.\nScale votes threshold.\nScale votes threshold.\nScale votes threshold.\nScale votes threshold.\nScale votes threshold.\nset template to search\nset template to search\nset template to search\nset template to search\nset template to search\nC++ default parameters\nC++ default parameters\nC++ default parameters\nC++ default parameters\nC++ default parameters\nset template to search\nset template to search\nset template to search\nset template to search\nset template to search\nNote\nNote\nNote\nNote\nNote\nSets size of grid for histogram equalization. Input image …\nSets size of grid for histogram equalization. Input image …\nSets size of grid for histogram equalization. Input image …\nSets size of grid for histogram equalization. Input image …\nSets size of grid for histogram equalization. Input image …\nThe accumulator threshold for the template centers at the …\nThe accumulator threshold for the template centers at the …\nThe accumulator threshold for the template centers at the …\nThe accumulator threshold for the template centers at the …\nThe accumulator threshold for the template centers at the …\nSpecify weights of feature functions\nSpecify weights of feature functions\nSpecify weights of feature functions\nSpecify weights of feature functions\nSpecify weights of feature functions\nAngle difference in degrees between two points in feature.\nAngle difference in degrees between two points in feature.\nAngle difference in degrees between two points in feature.\nAngle difference in degrees between two points in feature.\nAngle difference in degrees between two points in feature.\nCalculates the first, second, third, or mixed image …\nCalculates the first, second, third, or mixed image …\nCalculates the first order image derivative in both x and …\nCalculates the first order image derivative in both x and …\nCalculates the normalized sum of squares of the pixel …\nCalculates the normalized sum of squares of the pixel …\nApplies a fixed-level threshold to each array element.\nreturns pointer to the current pixel\nreturns pointer to the current pixel\nreturns pointer to the current pixel\nreturns pointer to the current pixel\nreturns pointer to the current pixel\nApplies an affine transformation to an image.\nApplies an affine transformation to an image.\nApplies a perspective transformation to an image.\nApplies a perspective transformation to an image.\n\\brief Remaps an image to polar or semilog-polar …\nPerforms a marker-based image segmentation using the …\nAndroid - not used\nAndroid - not used\nAuto detect == 0\nAuto detect == 0\nAravis SDK\nAravis SDK\nAVFoundation framework for iOS (OS X Lion will have the …\nAVFoundation framework for iOS (OS X Lion will have the …\nSame value as CAP_FIREWIRE\nSame value as CAP_FIREWIRE\nDirectShow (via videoInput)\nDirectShow (via videoInput)\nOpen and record video file or stream using the FFMPEG …\nOpen and record video file or stream using the FFMPEG …\nSame value as CAP_FIREWIRE\nIEEE 1394 drivers\nIEEE 1394 drivers\nSmartek Giganetix GigEVisionSDK\nSmartek Giganetix GigEVisionSDK\ngPhoto2 connection\ngPhoto2 connection\nGStreamer\nGStreamer\nSame value as CAP_FIREWIRE\nOpenCV Image Sequence (e.g. img_%02d.jpg)\nOpenCV Image Sequence (e.g. img_%02d.jpg)\nRealSense (former Intel Perceptual Computing SDK)\nRealSense (former Intel Perceptual Computing SDK)\nEach pixel is a 16-bit integer. The value indicates the …\nEach pixel is a 16-bit integer. The value indicates the …\nEach pixel contains two 32-bit floating point values in …\nIntel MediaSDK\nIntel MediaSDK\nMicrosoft Media Foundation (via videoInput)\nMicrosoft Media Foundation (via videoInput)\nBuilt-in OpenCV MotionJPEG codec\nBuilt-in OpenCV MotionJPEG codec\nOpenNI (for Kinect)\nOpenNI (for Kinect)\nOpenNI2 (for Kinect)\nOpenNI2 (for Kinect)\nOpenNI2 (for Orbbec Astra)\nOpenNI2 (for Orbbec Astra)\nOpenNI2 (for Asus Xtion and Occipital Structure sensors)\nOpenNI2 (for Asus Xtion and Occipital Structure sensors)\nOpenNI (for Asus Xtion)\nOpenNI (for Asus Xtion)\nData given from RGB image generator\nDepth values in mm (CV_16UC1)\nDisparity in pixels (CV_8UC1)\nDisparity in pixels (CV_32FC1)\nData given from RGB image generator\nData given from IR image generator\nXYZ in meters (CV_32FC3)\nCV_8UC1\nAperture. Can be readonly, depends on camera program.\nAutomatically trigger frame capture if camera is …\nDC1394: exposure control done by camera, user can adjust …\nDC1394: exposure control done by camera, user can adjust …\nenable/ disable auto white-balance\nenable/ disable auto white-balance\nCurrent backend (enum VideoCaptureAPIs). Read-only property\nCurrent backend (enum VideoCaptureAPIs). Read-only property\n(read-only) Video bitrate in kbits/s\n(read-only) Video bitrate in kbits/s\nBrightness of the image (only for those cameras that …\nBrightness of the image (only for those cameras that …\nVideo input or Channel Number (only for those cameras that …\nVideo input or Channel Number (only for those cameras that …\n(read-only) codec’s pixel format. 4-character code - see …\n(read-only) codec’s pixel format. 4-character code - see …\nContrast of the image (only for cameras).\nContrast of the image (only for cameras).\nBoolean flags indicating whether images should be …\nBoolean flags indicating whether images should be …\nset automatically when a value of the feature is set by …\nturn the feature off (not controlled manually nor …\nExposure (only for those cameras that support).\nExposure (only for those cameras that support).\nCamera exposure program.\nFormat of the %Mat objects (see Mat::type()) returned by …\nFormat of the %Mat objects (see Mat::type()) returned by …\n4-character code of codec. see VideoWriter::fourcc .\n4-character code of codec. see VideoWriter::fourcc .\nFrame rate.\nFrame rate.\nNumber of frames in the video file.\nNumber of frames in the video file.\nHeight of the frames in the video stream.\nHeight of the frames in the video stream.\nWidth of the frames in the video stream.\nWidth of the frames in the video stream.\nGain of the image (only for those cameras that support).\nGain of the image (only for those cameras that support).\nCollect messages with details.\nReadonly, returns (const char *).\nCapture only preview from liveview mode.\nTrigger, only by set. Reload camera settings.\nReload all settings on set.\nReadonly, returns (const char *).\nDefault is 1\nHue of the image (only for cameras).\nHue of the image (only for cameras).\n(<strong>open-only</strong>) Hardware acceleration type (see …\n(<strong>open-only</strong>) Hardware acceleration type (see …\n(<strong>open-only</strong>) If non-zero, create new OpenCL context and …\n(<strong>open-only</strong>) If non-zero, create new OpenCL context and …\n(<strong>open-only</strong>) Hardware device index (select GPU if multiple …\n(<strong>open-only</strong>) Hardware device index (select GPU if multiple …\nBackend-specific value indicating the current capture mode.\nBackend-specific value indicating the current capture mode.\nIn mm\nIn pixels\nIn mm\nFlag that synchronizes the remapping depth map to image map\n(<strong>open-only</strong>) timeout in milliseconds for opening a video …\n(<strong>open-only</strong>) timeout in milliseconds for opening a video …\nif true - rotates output frames of CvCapture considering …\nif true - rotates output frames of CvCapture considering …\n(read-only) Frame rotation defined by stream meta …\n(read-only) Frame rotation defined by stream meta …\nRelative position of the video file: 0=start of the film, …\nRelative position of the video file: 0=start of the film, …\n0-based index of the frame to be decoded/captured next.\n0-based index of the frame to be decoded/captured next.\nCurrent position of the video file in milliseconds.\nCurrent position of the video file in milliseconds.\nHorizontal binning factor.\nVertical binning factor.\nHorizontal sub-sampling of the image.\nVertical sub-sampling of the image.\nFrameStartTriggerMode: Determines how a frame is initiated.\nIP for enable multicast master mode. 0 for disable …\nPixel format.\n(<strong>open-only</strong>) timeout in milliseconds for reading from a …\n(<strong>open-only</strong>) timeout in milliseconds for reading from a …\nRectification flag for stereo cameras (note: only …\nRectification flag for stereo cameras (note: only …\nSample aspect ratio: num/den (den)\nSample aspect ratio: num/den (den)\nSample aspect ratio: num/den (num)\nSample aspect ratio: num/den (num)\nSaturation of the image (only for cameras).\nSaturation of the image (only for cameras).\nPop up video/camera filter dialog (note: only supported by …\nPop up video/camera filter dialog (note: only supported by …\nExposure speed. Can be readonly, depends on camera program.\nEnter liveview mode.\nwhite-balance color temperature\nwhite-balance color temperature\nCurrently unsupported.\nCurrently unsupported.\nAcquisition buffer size in buffer_size_unit. Default bytes.\nAcquisition buffer size unit in bytes. Default 1. E.g. …\nSets number of frames acquired by burst. This burst is …\nType of sensor frames timing.\nNumber of buffers to commit to low level.\nAcquisition transport buffer size in bytes.\nAutomatic exposure/gain.\nAverage intensity of output signal AEAG should achieve(in …\nAutomatic exposure/gain ROI Height.\nAutomatic exposure/gain ROI offset X.\nAutomatic exposure/gain ROI offset Y.\nAutomatic exposure/gain ROI Width.\nMaximum limit of exposure in AEAG procedure.\nMaximum limit of gain in AEAG procedure.\nEnable applying of CMS profiles to xiGetImage (see …\nAutomatic bandwidth calculation.\nAutomatic white balance.\nCalculate and returns available interface bandwidth(int …\nHorizontal Binning - number of horizontal photo-sensitive …\nBinning pattern type.\nBinning engine selector.\nVertical Binning - number of vertical photo-sensitive …\nCorrection of bad pixels.\nQueue of field/frame buffers.\nData move policy.\nColor Correction Matrix element [0][0].\nColor Correction Matrix element [0][1].\nColor Correction Matrix element [0][2].\nColor Correction Matrix element [0][3].\nColor Correction Matrix element [1][0].\nColor Correction Matrix element [1][1].\nColor Correction Matrix element [1][2].\nColor Correction Matrix element [1][3].\nColor Correction Matrix element [2][0].\nColor Correction Matrix element [2][1].\nColor Correction Matrix element [2][2].\nColor Correction Matrix element [2][3].\nColor Correction Matrix element [3][0].\nColor Correction Matrix element [3][1].\nColor Correction Matrix element [3][2].\nColor Correction Matrix element [3][3].\nCamera sensor temperature.\nMode of color management system.\nReturns color filter array type of RAW data.\nCorrection of column FPN.\nStart camera cooling.\nSelect counter.\nCounter status.\nOutput data format.\nEnable/Disable debounce to selected GPI.\nDebounce polarity (pol = 1 t0 - falling edge, t1 - rising …\nDebounce time (x * 10us).\nDebounce time (x * 10us).\nSet debug level.\nHorizontal Decimation - horizontal sub-sampling of the …\nDecimation pattern type.\nDecimation engine selector.\nVertical Decimation - vertical sub-sampling of the image - …\nSet default Color Correction Matrix.\nReturns device model id.\nResets the camera to default state.\nReturns device serial number.\nChange image resolution by binning or skipping.\nChange image downsampling type.\nExposure time in microseconds.\nSets the number of times of exposure in one frame.\nExposure priority (0.5 - exposure 50%, gain 50%).\nSetting of key enables file operations on some cameras.\nFile number.\nSize of file.\nDefine framerate in Hz.\nSize of free camera FFS.\nGain in dB.\nGain selector for parameter Gain allows to select …\nChromaticity gamma.\nLuminosity gamma.\nGet general purpose level.\nSet general purpose input mode.\nSelects general purpose input.\nSet general purpose output mode.\nSelects general purpose output.\nEnable High Dynamic Range feature.\nThe number of kneepoints in the PWLR.\nPosition of first kneepoint(in % of XI_PRM_EXPOSURE).\nPosition of second kneepoint (in % of XI_PRM_EXPOSURE).\nHeight of the Image provided by the device (in pixels).\nCamera housing back side temperature.\nCamera housing temperature.\nReturns hardware revision number.\nLast image black level counts. Can be used for Offline …\nbitdepth of data returned by function xiGetImage.\nOutput data format.\nThe alpha channel of RGB32 output image format.\nReturns 1 for color cameras.\nBuffer size in bytes sufficient for output image returned …\nReturns 1 for cameras that support cooling.\nReturns 1 if camera connected and works properly.\nValue of first kneepoint (% of sensor saturation).\nValue of second kneepoint (% of sensor saturation).\nDefine camera signalling LED functionality.\nSelects camera signalling LED.\nCurrent lens aperture value in stops. Examples: 2.8, 4, …\nAllows access to lens feature value currently selected by …\nSelects the current feature which is accessible by …\nLens focal distance in mm.\nLens focus distance in cm.\nMoves lens focus motor by steps set in …\nLens current focus movement value to be used by …\nStatus of lens control interface. This shall be set to …\nSet/get bandwidth(datarate)(in Megabits).\nActivates LUT.\nControl the index (offset) of the coefficient to access in …\nValue at entry LUTIndex of the LUT.\nCalculates White Balance(must be called during …\nHorizontal offset from the origin to the area of interest …\nVertical offset from the origin to the area of interest …\nDevice output data bit depth.\nDevice output data packing (or grouping) enabled. Packing …\nData packing type. Some cameras supports only specific …\nGetImage returns most recent frame.\nActivates/deactivates Region selected by Region Selector.\nSelects Region in Multiple ROI which parameters are set by …\nCorrection of row FPN.\nCamera sensor board temperature.\nSensor clock frequency in Hz.\nSensor clock frequency index. Sensor with selected …\nSensor output data bit depth.\nSelects the current feature which is accessible by …\nAllows access to sensor feature value currently selected …\nCurrent sensor mode. Allows to select sensor mode by one …\nNumber of output channels from sensor used for data …\nNumber of taps.\nSharpness Strength.\nChange sensor shutter type(CMOS sensor).\nSet sensor target temperature for cooling.\nSelects which test pattern type is generated by the …\nSelects which test pattern generator is controlled by the …\nImage capture timeout in milliseconds.\nCurrent format of pixels on transport layer.\nSpecifies the delay in microseconds (us) to apply after …\nSelects the type of trigger.\nGenerates an internal trigger. PRM_TRG_SOURCE must be set …\nDefines source of trigger.\nDefines how time stamp reset engine will be armed.\nDefines which source will be used for timestamp reset. …\nSize of used camera FFS.\nWhite balance blue coefficient.\nWhite balance green coefficient.\nWhite balance red coefficient.\nWidth of the Image provided by the device (in pixels).\nPvAPI, Prosilica GigE SDK\nPvAPI, Prosilica GigE SDK\n2 out of 16 decimation\n2 out of 4 decimation\n2 out of 8 decimation\nOff\nFixedRate\nFreerun\nSoftware\nSyncIn1\nSyncIn2\nBayer16\nBayer8\nBgr24\nBgra32\nMono16\nMono8\nRgb24\nRgba32\nQuickTime (obsolete, removed)\nQuickTime (obsolete, removed)\nSynonym for CAP_INTELPERC\nuEye Camera API\nuEye Camera API\nUnicap drivers (obsolete, removed)\nUnicap drivers (obsolete, removed)\nV4L/V4L2 capturing support\nSame as CAP_V4L\nVideo For Windows (obsolete, removed)\nVideo For Windows (obsolete, removed)\nMicrosoft Windows Runtime using Media Foundation\nMicrosoft Windows Runtime using Media Foundation\nXIMEA Camera API\nXIMEA Camera API\nXINE engine (Linux)\nXINE engine (Linux)\nDefaults to CV_8U.\nDefaults to CV_8U.\n(Read-only): Size of just encoded video frame. Note that …\n(Read-only): Size of just encoded video frame. Note that …\n(<strong>open-only</strong>) Hardware acceleration type (see …\n(<strong>open-only</strong>) Hardware acceleration type (see …\n(<strong>open-only</strong>) If non-zero, create new OpenCL context and …\n(<strong>open-only</strong>) If non-zero, create new OpenCL context and …\n(<strong>open-only</strong>) Hardware device index (select GPU if multiple …\n(<strong>open-only</strong>) Hardware device index (select GPU if multiple …\nIf it is not zero, the encoder will expect and encode …\nIf it is not zero, the encoder will expect and encode …\nNumber of stripes for parallel encoding. -1 for auto …\nNumber of stripes for parallel encoding. -1 for auto …\nCurrent quality (0..100%) of the encoded videostream. Can …\nCurrent quality (0..100%) of the encoded videostream. Can …\nPrefer to use H/W acceleration. If no one supported, then …\nPrefer to use H/W acceleration. If no one supported, then …\nDirectX 11\nDirectX 11\nlibmfx (Intel MediaSDK/oneVPL)\nlibmfx (Intel MediaSDK/oneVPL)\nDo not require any specific H/W acceleration, prefer …\nDo not require any specific H/W acceleration, prefer …\nVAAPI\nVAAPI\nVideo Acceleration type\nClass for video capturing from video files, image …\ncv::VideoCapture API backends identifier.\ncv::VideoCapture generic properties identifier.\nMutable methods for crate::videoio::VideoCapture\nConstant methods for crate::videoio::VideoCapture\nVideo writer class.\ncv::VideoWriter generic properties identifier.\nMutable methods for crate::videoio::VideoWriter\nConstant methods for crate::videoio::VideoWriter\nDefault constructor\nDefault constructors\nConcatenates 4 chars to a fourcc code\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nReturns the argument unchanged.\nDefault constructor\n@overload Opens a video file or a capturing device or an …\nDefault constructor\nReturns the specified VideoCapture property\nReturns the specified VideoCapture property\nReturns the specified VideoCapture property\nReturns the specified VideoCapture property\nReturns the specified VideoCapture property\nReturns the specified VideoWriter property\nReturns the specified VideoWriter property\nReturns the specified VideoWriter property\nReturns the specified VideoWriter property\nReturns the specified VideoWriter property\nReturns backend API name or “UnknownVideoAPI(xxx)”\nReturns used backend API name\nReturns used backend API name\nReturns used backend API name\nReturns used backend API name\nReturns used backend API name\nReturns used backend API name\nReturns used backend API name\nReturns used backend API name\nReturns used backend API name\nReturns used backend API name\nReturns list of all available backends\nReturns description and ABI/API version of videoio plugin…\nReturns list of available backends which works via …\nquery if exception mode is active\nquery if exception mode is active\nquery if exception mode is active\nquery if exception mode is active\nquery if exception mode is active\nReturns description and ABI/API version of videoio plugin…\nReturns list of available backends which works via …\nReturns description and ABI/API version of videoio plugin…\nReturns list of available backends which works via …\nGrabs the next frame from video file or capturing device.\nGrabs the next frame from video file or capturing device.\nGrabs the next frame from video file or capturing device.\nGrabs the next frame from video file or capturing device.\nGrabs the next frame from video file or capturing device.\nReturns true if backend is available\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nCalls <code>U::from(self)</code>.\nReturns true if backend is built in (false if backend is …\nReturns true if video capturing has been initialized …\nReturns true if video capturing has been initialized …\nReturns true if video capturing has been initialized …\nReturns true if video capturing has been initialized …\nReturns true if video capturing has been initialized …\nReturns true if video writer has been successfully …\nReturns true if video writer has been successfully …\nReturns true if video writer has been successfully …\nReturns true if video writer has been successfully …\nReturns true if video writer has been successfully …\nDefault constructor\nDefault constructors\nDefault constructors\nDefault constructors\n@overload Opens a camera for video capturing\n@overload\nDefault constructors\n@overload The <code>apiPreference</code> parameter allows to specify …\nDefault constructor\nOpens a camera for video capturing\nOpens a camera for video capturing\nOpens a camera for video capturing\nOpens a camera for video capturing\nOpens a camera for video capturing\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nOpens a camera for video capturing\nOpens a camera for video capturing\nOpens a camera for video capturing\nOpens a camera for video capturing\nOpens a camera for video capturing\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nOpens a video file or a capturing device or an IP video …\nOpens a video file or a capturing device or an IP video …\nOpens a video file or a capturing device or an IP video …\nOpens a video file or a capturing device or an IP video …\nOpens a video file or a capturing device or an IP video …\nOpens a video file or a capturing device or an IP video …\nOpens a video file or a capturing device or an IP video …\nOpens a video file or a capturing device or an IP video …\nOpens a video file or a capturing device or an IP video …\nOpens a video file or a capturing device or an IP video …\nOpens a camera for video capturing\nOpens a camera for video capturing\nOpens a camera for video capturing\nOpens a camera for video capturing\nOpens a camera for video capturing\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\nInitializes or reinitializes video writer.\n@overload\n@overload\n@overload\n@overload\n@overload\nReturns true if video capturing has been initialized …\nReturns true if video capturing has been initialized …\nReturns true if video capturing has been initialized …\nReturns true if video capturing has been initialized …\nReturns true if video capturing has been initialized …\nGrabs, decodes and returns the next video frame.\nGrabs, decodes and returns the next video frame.\nGrabs, decodes and returns the next video frame.\nGrabs, decodes and returns the next video frame.\nGrabs, decodes and returns the next video frame.\nCloses video file or capturing device.\nCloses video file or capturing device.\nCloses video file or capturing device.\nCloses video file or capturing device.\nCloses video file or capturing device.\nCloses the video writer.\nCloses the video writer.\nCloses the video writer.\nCloses the video writer.\nCloses the video writer.\nDecodes and returns the grabbed video frame.\nDecodes and returns the grabbed video frame.\nDecodes and returns the grabbed video frame.\nDecodes and returns the grabbed video frame.\nDecodes and returns the grabbed video frame.\nDecodes and returns the grabbed video frame.\nDecodes and returns the grabbed video frame.\nDecodes and returns the grabbed video frame.\nDecodes and returns the grabbed video frame.\nDecodes and returns the grabbed video frame.\nSets a property in the VideoCapture.\nSets a property in the VideoCapture.\nSets a property in the VideoCapture.\nSets a property in the VideoCapture.\nSets a property in the VideoCapture.\nSets a property in the VideoWriter.\nSets a property in the VideoWriter.\nSets a property in the VideoWriter.\nSets a property in the VideoWriter.\nSets a property in the VideoWriter.\nSwitches exceptions mode\nSwitches exceptions mode\nSwitches exceptions mode\nSwitches exceptions mode\nSwitches exceptions mode\nWait for ready frames from VideoCapture.\nWait for ready frames from VideoCapture.\nWrites the next video frame\nWrites the next video frame\nWrites the next video frame\nWrites the next video frame\nWrites the next video frame\nReturns the argument unchanged.\nCalls <code>U::from(self)</code>.\nMutable methods for crate::dnn::AbsLayer\nConstant methods for crate::dnn::AbsLayer\nMutable methods for crate::dnn::AccumLayer\nConstant methods for crate::dnn::AccumLayer\nMutable methods for crate::dnn::ActivationLayerInt8\nConstant methods for crate::dnn::ActivationLayerInt8\nMutable methods for crate::dnn::ActivationLayer\nConstant methods for crate::dnn::ActivationLayer\nMutable methods for core::Algorithm\nConstant methods for core::Algorithm\nMutable methods for core::Arrays\nConstant methods for core::Arrays\nMutable methods for core::AsyncArray\nConstant methods for core::AsyncArray\nMutable methods for core::AsyncPromise\nConstant methods for core::AsyncPromise\nMutable methods for crate::dnn::BNLLLayer\nConstant methods for crate::dnn::BNLLLayer\nMutable methods for crate::dnn::BackendNode\nConstant methods for crate::dnn::BackendNode\nMutable methods for crate::dnn::BackendWrapper\nConstant methods for crate::dnn::BackendWrapper\nMutable methods for crate::dnn::BaseConvolutionLayer\nConstant methods for crate::dnn::BaseConvolutionLayer\nMutable methods for crate::dnn::BatchNormLayerInt8\nConstant methods for crate::dnn::BatchNormLayerInt8\nMutable methods for crate::dnn::BatchNormLayer\nConstant methods for crate::dnn::BatchNormLayer\nMutable methods for crate::dnn::BlankLayer\nConstant methods for crate::dnn::BlankLayer\nMutable methods for core::BufferPool\nConstant methods for core::BufferPool\nMutable methods for core::Buffer\nConstant methods for core::Buffer\nMutable methods for crate::imgproc::CLAHE\nConstant methods for crate::imgproc::CLAHE\nMutable methods for crate::dnn::ChannelsPReLULayer\nConstant methods for crate::dnn::ChannelsPReLULayer\nMutable methods for crate::dnn::ClassificationModel\nConstant methods for crate::dnn::ClassificationModel\nMutable methods for core::CommandLineParser\nConstant methods for core::CommandLineParser\nMutable methods for crate::dnn::ConcatLayer\nConstant methods for crate::dnn::ConcatLayer\nMutable methods for core::ConjGradSolver\nConstant methods for core::ConjGradSolver\nMutable methods for crate::dnn::ConstLayer\nConstant methods for crate::dnn::ConstLayer\nMutable methods for core::Context\nConstant methods for core::Context\nMutable methods for core::Context_UserContext\nConstant methods for core::Context_UserContext\nMutable methods for crate::dnn::ConvolutionLayerInt8\nConstant methods for crate::dnn::ConvolutionLayerInt8\nMutable methods for crate::dnn::ConvolutionLayer\nConstant methods for crate::dnn::ConvolutionLayer\nMutable methods for crate::dnn::CorrelationLayer\nConstant methods for crate::dnn::CorrelationLayer\nMutable methods for crate::dnn::CropAndResizeLayer\nConstant methods for crate::dnn::CropAndResizeLayer\nMutable methods for crate::dnn::CropLayer\nConstant methods for crate::dnn::CropLayer\nMutable methods for crate::dnn::CumSumLayer\nConstant methods for crate::dnn::CumSumLayer\nMutable methods for crate::dnn::DataAugmentationLayer\nConstant methods for crate::dnn::DataAugmentationLayer\nImplement this trait types that are valid to use as Mat …\nMutable methods for crate::dnn::DeconvolutionLayer\nConstant methods for crate::dnn::DeconvolutionLayer\nMutable methods for crate::dnn::DequantizeLayer\nConstant methods for crate::dnn::DequantizeLayer\nMutable methods for core::Detail_CheckContext\nConstant methods for core::Detail_CheckContext\nMutable methods for crate::dnn::DetectionModel\nConstant methods for crate::dnn::DetectionModel\nMutable methods for crate::dnn::DetectionOutputLayer\nConstant methods for crate::dnn::DetectionOutputLayer\nMutable methods for core::DeviceInfo\nConstant methods for core::DeviceInfo\nMutable methods for core::Device\nConstant methods for core::Device\nMutable methods for crate::dnn::Dict\nConstant methods for crate::dnn::Dict\nMutable methods for crate::dnn::DictValue\nConstant methods for crate::dnn::DictValue\nMutable methods for core::DownhillSolver\nConstant methods for core::DownhillSolver\nMutable methods for crate::dnn::ELULayer\nConstant methods for crate::dnn::ELULayer\nMutable methods for crate::dnn::EltwiseLayerInt8\nConstant methods for crate::dnn::EltwiseLayerInt8\nMutable methods for crate::dnn::EltwiseLayer\nConstant methods for crate::dnn::EltwiseLayer\nMutable methods for core::Event\nConstant methods for core::Event\nMutable methods for core::Exception\nConstant methods for core::Exception\nMutable methods for crate::dnn::ExpLayer\nConstant methods for crate::dnn::ExpLayer\nMutable methods for core::FileNodeIterator\nConstant methods for core::FileNodeIterator\nMutable methods for core::FileNode\nConstant methods for core::FileNode\nMutable methods for core::FileStorage\nConstant methods for core::FileStorage\nMutable methods for crate::dnn::FlattenLayer\nConstant methods for crate::dnn::FlattenLayer\nMutable methods for crate::dnn::FlowWarpLayer\nConstant methods for crate::dnn::FlowWarpLayer\nMutable methods for core::Formatted\nConstant methods for core::Formatted\nMutable methods for core::Formatter\nConstant methods for core::Formatter\nMutable methods for crate::dnn::GRULayer\nConstant methods for crate::dnn::GRULayer\nMutable methods for crate::imgproc::GeneralizedHoughBallard\nConstant methods for …\nMutable methods for crate::imgproc::GeneralizedHoughGuil\nConstant methods for crate::imgproc::GeneralizedHoughGuil\nMutable methods for crate::imgproc::GeneralizedHough\nConstant methods for crate::imgproc::GeneralizedHough\nMutable methods for core::GpuData\nConstant methods for core::GpuData\nMutable methods for core::GpuMatND\nConstant methods for core::GpuMatND\nMutable methods for core::GpuMat\nConstant methods for core::GpuMat\nMutable methods for core::GpuMat_Allocator\nConstant methods for core::GpuMat_Allocator\nMutable methods for core::Hamming\nConstant methods for core::Hamming\nMutable methods for core::HostMem\nConstant methods for core::HostMem\nMutable methods for core::Image2D\nConstant methods for core::Image2D\nMutable methods for crate::dnn::InnerProductLayerInt8\nConstant methods for crate::dnn::InnerProductLayerInt8\nMutable methods for crate::dnn::InnerProductLayer\nConstant methods for crate::dnn::InnerProductLayer\nMutable methods for crate::imgproc::IntelligentScissorsMB\nConstant methods for crate::imgproc::IntelligentScissorsMB\nMutable methods for crate::dnn::InterpLayer\nConstant methods for crate::dnn::InterpLayer\nMutable methods for core::KernelArg\nConstant methods for core::KernelArg\nMutable methods for core::Kernel\nConstant methods for core::Kernel\nMutable methods for core::KeyPoint\nConstant methods for core::KeyPoint\nMutable methods for crate::dnn::KeypointsModel\nConstant methods for crate::dnn::KeypointsModel\nMutable methods for core::LDA\nConstant methods for core::LDA\nMutable methods for crate::dnn::LRNLayer\nConstant methods for crate::dnn::LRNLayer\nMutable methods for crate::dnn::LSTMLayer\nConstant methods for crate::dnn::LSTMLayer\nMutable methods for crate::dnn::LayerFactory\nConstant methods for crate::dnn::LayerFactory\nMutable methods for crate::dnn::LayerParams\nConstant methods for crate::dnn::LayerParams\nMutable methods for crate::dnn::Layer\nConstant methods for crate::dnn::Layer\nMutable methods for crate::imgproc::LineIterator\nConstant methods for crate::imgproc::LineIterator\nMutable methods for crate::imgproc::LineSegmentDetector\nConstant methods for crate::imgproc::LineSegmentDetector\nMutable methods for core::LogTag\nConstant methods for core::LogTag\nMutable methods for crate::dnn::MVNLayer\nConstant methods for crate::dnn::MVNLayer\nn-dimensional dense array class \\anchor CVMat_Details\nMutable methods for core::MatConstIterator\nConstant methods for core::MatConstIterator\nMutable methods for core::MatExpr\nConstant methods for core::MatExpr\nMutable methods for core::MatOp\nConstant methods for core::MatOp\nMutable methods for core::MatSize\nConstant methods for core::MatSize\nMutable methods for core::MatStep\nConstant methods for core::MatStep\nMutable methods for core::Mat\nConstant methods for core::Mat\nMutable methods for core::Matx_AddOp\nConstant methods for core::Matx_AddOp\nMutable methods for core::Matx_DivOp\nConstant methods for core::Matx_DivOp\nMutable methods for core::Matx_MatMulOp\nConstant methods for core::Matx_MatMulOp\nMutable methods for core::Matx_MulOp\nConstant methods for core::Matx_MulOp\nMutable methods for core::Matx_ScaleOp\nConstant methods for core::Matx_ScaleOp\nMutable methods for core::Matx_SubOp\nConstant methods for core::Matx_SubOp\nMutable methods for core::Matx_TOp\nConstant methods for core::Matx_TOp\nMutable methods for crate::dnn::MaxUnpoolLayer\nConstant methods for crate::dnn::MaxUnpoolLayer\nMutable methods for core::MinProblemSolver\nConstant methods for core::MinProblemSolver\nMutable methods for core::MinProblemSolver_Function\nConstant methods for core::MinProblemSolver_Function\nMutable methods for crate::dnn::MishLayer\nConstant methods for crate::dnn::MishLayer\nMutable methods for crate::dnn::Model\nConstant methods for crate::dnn::Model\nMutable methods for crate::dnn::Net\nConstant methods for crate::dnn::Net\nMutable methods for core::NodeData\nConstant methods for core::NodeData\nMutable methods for crate::dnn::NormalizeBBoxLayer\nConstant methods for crate::dnn::NormalizeBBoxLayer\nMutable methods for core::OpenCLExecutionContext\nConstant methods for core::OpenCLExecutionContext\nMutable methods for core::PCA\nConstant methods for core::PCA\nMutable methods for crate::dnn::PaddingLayer\nConstant methods for crate::dnn::PaddingLayer\nMutable methods for core::ParallelLoopBody\nConstant methods for core::ParallelLoopBody\nMutable methods for crate::dnn::PermuteLayer\nConstant methods for crate::dnn::PermuteLayer\nMutable methods for core::PlatformInfo\nConstant methods for core::PlatformInfo\nMutable methods for core::Platform\nConstant methods for core::Platform\nMutable methods for crate::dnn::PoolingLayerInt8\nConstant methods for crate::dnn::PoolingLayerInt8\nMutable methods for crate::dnn::PoolingLayer\nConstant methods for crate::dnn::PoolingLayer\nMutable methods for crate::dnn::PowerLayer\nConstant methods for crate::dnn::PowerLayer\nMutable methods for crate::dnn::PriorBoxLayer\nConstant methods for crate::dnn::PriorBoxLayer\nMutable methods for core::ProgramSource\nConstant methods for core::ProgramSource\nMutable methods for core::Program\nConstant methods for core::Program\nMutable methods for crate::dnn::ProposalLayer\nConstant methods for crate::dnn::ProposalLayer\nMutable methods for crate::dnn::QuantizeLayer\nConstant methods for crate::dnn::QuantizeLayer\nMutable methods for core::Queue\nConstant methods for core::Queue\nMutable methods for core::RNG\nConstant methods for core::RNG\nMutable methods for core::RNG_MT19937\nConstant methods for core::RNG_MT19937\nMutable methods for crate::dnn::RNNLayer\nConstant methods for crate::dnn::RNNLayer\nMutable methods for core::Range\nConstant methods for core::Range\nMutable methods for crate::dnn::ReLU6Layer\nConstant methods for crate::dnn::ReLU6Layer\nMutable methods for crate::dnn::ReLULayer\nConstant methods for crate::dnn::ReLULayer\nMutable methods for crate::dnn::RegionLayer\nConstant methods for crate::dnn::RegionLayer\nMutable methods for crate::dnn::ReorgLayer\nConstant methods for crate::dnn::ReorgLayer\nMutable methods for crate::dnn::RequantizeLayer\nConstant methods for crate::dnn::RequantizeLayer\nMutable methods for crate::dnn::ReshapeLayer\nConstant methods for crate::dnn::ReshapeLayer\nMutable methods for crate::dnn::ResizeLayer\nConstant methods for crate::dnn::ResizeLayer\nMutable methods for core::SVD\nConstant methods for core::SVD\nMutable methods for crate::dnn::ScaleLayerInt8\nConstant methods for crate::dnn::ScaleLayerInt8\nMutable methods for crate::dnn::ScaleLayer\nConstant methods for crate::dnn::ScaleLayer\nMutable methods for crate::dnn::SegmentationModel\nConstant methods for crate::dnn::SegmentationModel\nMutable methods for crate::dnn::ShiftLayerInt8\nConstant methods for crate::dnn::ShiftLayerInt8\nMutable methods for crate::dnn::ShiftLayer\nConstant methods for crate::dnn::ShiftLayer\nMutable methods for crate::dnn::ShuffleChannelLayer\nConstant methods for crate::dnn::ShuffleChannelLayer\nMutable methods for crate::dnn::SigmoidLayer\nConstant methods for crate::dnn::SigmoidLayer\nMutable methods for crate::dnn::SliceLayer\nConstant methods for crate::dnn::SliceLayer\nMutable methods for crate::dnn::SoftmaxLayerInt8\nConstant methods for crate::dnn::SoftmaxLayerInt8\nMutable methods for crate::dnn::SoftmaxLayer\nConstant methods for crate::dnn::SoftmaxLayer\nMutable methods for core::SparseMatConstIterator\nConstant methods for core::SparseMatConstIterator\nMutable methods for core::SparseMatIterator\nConstant methods for core::SparseMatIterator\nMutable methods for core::SparseMat\nConstant methods for core::SparseMat\nMutable methods for core::SparseMat_Hdr\nConstant methods for core::SparseMat_Hdr\nMutable methods for core::SparseMat_Node\nConstant methods for core::SparseMat_Node\nMutable methods for crate::dnn::SplitLayer\nConstant methods for crate::dnn::SplitLayer\nMutable methods for core::Stream\nConstant methods for core::Stream\nMutable methods for crate::imgproc::Subdiv2D\nConstant methods for crate::imgproc::Subdiv2D\nMutable methods for crate::dnn::SwishLayer\nConstant methods for crate::dnn::SwishLayer\nMutable methods for crate::dnn::TanHLayer\nConstant methods for crate::dnn::TanHLayer\nMutable methods for core::TargetArchs\nConstant methods for core::TargetArchs\nMutable methods for crate::dnn::TextDetectionModel\nConstant methods for crate::dnn::TextDetectionModel\nMutable methods for crate::dnn::TextDetectionModel_DB\nConstant methods for crate::dnn::TextDetectionModel_DB\nMutable methods for crate::dnn::TextDetectionModel_EAST\nConstant methods for crate::dnn::TextDetectionModel_EAST\nMutable methods for crate::dnn::TextRecognitionModel\nConstant methods for crate::dnn::TextRecognitionModel\nMutable methods for core::Texture2D\nConstant methods for core::Texture2D\nMutable methods for core::TickMeter\nConstant methods for core::TickMeter\nMutable methods for core::Timer\nConstant methods for core::Timer\nMutable methods for core::UMatData\nConstant methods for core::UMatData\nMutable methods for core::UMat\nConstant methods for core::UMat\nMutable methods for crate::videoio::VideoCapture\nConstant methods for crate::videoio::VideoCapture\nMutable methods for crate::videoio::VideoWriter\nConstant methods for crate::videoio::VideoWriter\nMutable methods for core::WriteStructContext\nConstant methods for core::WriteStructContext\nMutable methods for core::_InputArray\nConstant methods for core::_InputArray\nMutable methods for core::_InputOutputArray\nConstant methods for core::_InputOutputArray\nMutable methods for core::_OutputArray\nConstant methods for core::_OutputArray\nMutable methods for crate::dnn::_Range\nConstant methods for crate::dnn::_Range\nHelper function to call OpenCV functions that allow …\nAmount of layers/channels per element. E.g. for an 8-bit …\nThe shape of bytes occupied by the single layer/channel of …\nConvert <code>Vector</code> to <code>Vec</code>\nThe return type of this function goes into …\nThe return type of this function goes into <code>receive_string</code>\nUsed for both regular <code>String</code> and byte string (<code>Vec&lt;u8&gt;</code>)\nTrait for structures that are created on the C++ side, …\nReturn the underlying raw pointer.\nReturn the underlying mutable raw pointer\nWrap the specified raw pointer\nReturn the underlying raw pointer while consuming this …\nTrait for structures that are created on the C++ side, …\nReturn the underlying raw pointer.\nReturn the underlying mutable raw pointer\nWrap the specified raw pointer\nReturn the underlying raw pointer while consuming this …\nAndroid - not used\nAndroid - not used\nAuto detect == 0\nAuto detect == 0\nAravis SDK\nAravis SDK\nAVFoundation framework for iOS (OS X Lion will have the …\nAVFoundation framework for iOS (OS X Lion will have the …\nSame value as CAP_FIREWIRE\nSame value as CAP_FIREWIRE\nDirectShow (via videoInput)\nDirectShow (via videoInput)\nOpen and record video file or stream using the FFMPEG …\nOpen and record video file or stream using the FFMPEG …\nSame value as CAP_FIREWIRE\nIEEE 1394 drivers\nIEEE 1394 drivers\nSmartek Giganetix GigEVisionSDK\nSmartek Giganetix GigEVisionSDK\ngPhoto2 connection\ngPhoto2 connection\nGStreamer\nGStreamer\nSame value as CAP_FIREWIRE\nOpenCV Image Sequence (e.g. img_%02d.jpg)\nOpenCV Image Sequence (e.g. img_%02d.jpg)\nRealSense (former Intel Perceptual Computing SDK)\nRealSense (former Intel Perceptual Computing SDK)\nEach pixel is a 16-bit integer. The value indicates the …\nEach pixel is a 16-bit integer. The value indicates the …\nEach pixel contains two 32-bit floating point values in …\nIntel MediaSDK\nIntel MediaSDK\nMicrosoft Media Foundation (via videoInput)\nMicrosoft Media Foundation (via videoInput)\nBuilt-in OpenCV MotionJPEG codec\nBuilt-in OpenCV MotionJPEG codec\nOpenNI (for Kinect)\nOpenNI (for Kinect)\nOpenNI2 (for Kinect)\nOpenNI2 (for Kinect)\nOpenNI2 (for Orbbec Astra)\nOpenNI2 (for Orbbec Astra)\nOpenNI2 (for Asus Xtion and Occipital Structure sensors)\nOpenNI2 (for Asus Xtion and Occipital Structure sensors)\nOpenNI (for Asus Xtion)\nOpenNI (for Asus Xtion)\nData given from RGB image generator\nDepth values in mm (CV_16UC1)\nDisparity in pixels (CV_8UC1)\nDisparity in pixels (CV_32FC1)\nData given from RGB image generator\nData given from IR image generator\nXYZ in meters (CV_32FC3)\nCV_8UC1\nAperture. Can be readonly, depends on camera program.\nAutomatically trigger frame capture if camera is …\nDC1394: exposure control done by camera, user can adjust …\nDC1394: exposure control done by camera, user can adjust …\nenable/ disable auto white-balance\nenable/ disable auto white-balance\nCurrent backend (enum VideoCaptureAPIs). Read-only property\nCurrent backend (enum VideoCaptureAPIs). Read-only property\n(read-only) Video bitrate in kbits/s\n(read-only) Video bitrate in kbits/s\nBrightness of the image (only for those cameras that …\nBrightness of the image (only for those cameras that …\nVideo input or Channel Number (only for those cameras that …\nVideo input or Channel Number (only for those cameras that …\n(read-only) codec’s pixel format. 4-character code - see …\n(read-only) codec’s pixel format. 4-character code - see …\nContrast of the image (only for cameras).\nContrast of the image (only for cameras).\nBoolean flags indicating whether images should be …\nBoolean flags indicating whether images should be …\nset automatically when a value of the feature is set by …\nturn the feature off (not controlled manually nor …\nExposure (only for those cameras that support).\nExposure (only for those cameras that support).\nCamera exposure program.\nFormat of the %Mat objects (see Mat::type()) returned by …\nFormat of the %Mat objects (see Mat::type()) returned by …\n4-character code of codec. see VideoWriter::fourcc .\n4-character code of codec. see VideoWriter::fourcc .\nFrame rate.\nFrame rate.\nNumber of frames in the video file.\nNumber of frames in the video file.\nHeight of the frames in the video stream.\nHeight of the frames in the video stream.\nWidth of the frames in the video stream.\nWidth of the frames in the video stream.\nGain of the image (only for those cameras that support).\nGain of the image (only for those cameras that support).\nCollect messages with details.\nReadonly, returns (const char *).\nCapture only preview from liveview mode.\nTrigger, only by set. Reload camera settings.\nReload all settings on set.\nReadonly, returns (const char *).\nDefault is 1\nHue of the image (only for cameras).\nHue of the image (only for cameras).\n(<strong>open-only</strong>) Hardware acceleration type (see …\n(<strong>open-only</strong>) Hardware acceleration type (see …\n(<strong>open-only</strong>) If non-zero, create new OpenCL context and …\n(<strong>open-only</strong>) If non-zero, create new OpenCL context and …\n(<strong>open-only</strong>) Hardware device index (select GPU if multiple …\n(<strong>open-only</strong>) Hardware device index (select GPU if multiple …\nBackend-specific value indicating the current capture mode.\nBackend-specific value indicating the current capture mode.\nIn mm\nIn pixels\nIn mm\nFlag that synchronizes the remapping depth map to image map\n(<strong>open-only</strong>) timeout in milliseconds for opening a video …\n(<strong>open-only</strong>) timeout in milliseconds for opening a video …\nif true - rotates output frames of CvCapture considering …\nif true - rotates output frames of CvCapture considering …\n(read-only) Frame rotation defined by stream meta …\n(read-only) Frame rotation defined by stream meta …\nRelative position of the video file: 0=start of the film, …\nRelative position of the video file: 0=start of the film, …\n0-based index of the frame to be decoded/captured next.\n0-based index of the frame to be decoded/captured next.\nCurrent position of the video file in milliseconds.\nCurrent position of the video file in milliseconds.\nHorizontal binning factor.\nVertical binning factor.\nHorizontal sub-sampling of the image.\nVertical sub-sampling of the image.\nFrameStartTriggerMode: Determines how a frame is initiated.\nIP for enable multicast master mode. 0 for disable …\nPixel format.\n(<strong>open-only</strong>) timeout in milliseconds for reading from a …\n(<strong>open-only</strong>) timeout in milliseconds for reading from a …\nRectification flag for stereo cameras (note: only …\nRectification flag for stereo cameras (note: only …\nSample aspect ratio: num/den (den)\nSample aspect ratio: num/den (den)\nSample aspect ratio: num/den (num)\nSample aspect ratio: num/den (num)\nSaturation of the image (only for cameras).\nSaturation of the image (only for cameras).\nPop up video/camera filter dialog (note: only supported by …\nPop up video/camera filter dialog (note: only supported by …\nExposure speed. Can be readonly, depends on camera program.\nEnter liveview mode.\nwhite-balance color temperature\nwhite-balance color temperature\nCurrently unsupported.\nCurrently unsupported.\nAcquisition buffer size in buffer_size_unit. Default bytes.\nAcquisition buffer size unit in bytes. Default 1. E.g. …\nSets number of frames acquired by burst. This burst is …\nType of sensor frames timing.\nNumber of buffers to commit to low level.\nAcquisition transport buffer size in bytes.\nAutomatic exposure/gain.\nAverage intensity of output signal AEAG should achieve(in …\nAutomatic exposure/gain ROI Height.\nAutomatic exposure/gain ROI offset X.\nAutomatic exposure/gain ROI offset Y.\nAutomatic exposure/gain ROI Width.\nMaximum limit of exposure in AEAG procedure.\nMaximum limit of gain in AEAG procedure.\nEnable applying of CMS profiles to xiGetImage (see …\nAutomatic bandwidth calculation.\nAutomatic white balance.\nCalculate and returns available interface bandwidth(int …\nHorizontal Binning - number of horizontal photo-sensitive …\nBinning pattern type.\nBinning engine selector.\nVertical Binning - number of vertical photo-sensitive …\nCorrection of bad pixels.\nQueue of field/frame buffers.\nData move policy.\nColor Correction Matrix element [0][0].\nColor Correction Matrix element [0][1].\nColor Correction Matrix element [0][2].\nColor Correction Matrix element [0][3].\nColor Correction Matrix element [1][0].\nColor Correction Matrix element [1][1].\nColor Correction Matrix element [1][2].\nColor Correction Matrix element [1][3].\nColor Correction Matrix element [2][0].\nColor Correction Matrix element [2][1].\nColor Correction Matrix element [2][2].\nColor Correction Matrix element [2][3].\nColor Correction Matrix element [3][0].\nColor Correction Matrix element [3][1].\nColor Correction Matrix element [3][2].\nColor Correction Matrix element [3][3].\nCamera sensor temperature.\nMode of color management system.\nReturns color filter array type of RAW data.\nCorrection of column FPN.\nStart camera cooling.\nSelect counter.\nCounter status.\nOutput data format.\nEnable/Disable debounce to selected GPI.\nDebounce polarity (pol = 1 t0 - falling edge, t1 - rising …\nDebounce time (x * 10us).\nDebounce time (x * 10us).\nSet debug level.\nHorizontal Decimation - horizontal sub-sampling of the …\nDecimation pattern type.\nDecimation engine selector.\nVertical Decimation - vertical sub-sampling of the image - …\nSet default Color Correction Matrix.\nReturns device model id.\nResets the camera to default state.\nReturns device serial number.\nChange image resolution by binning or skipping.\nChange image downsampling type.\nExposure time in microseconds.\nSets the number of times of exposure in one frame.\nExposure priority (0.5 - exposure 50%, gain 50%).\nSetting of key enables file operations on some cameras.\nFile number.\nSize of file.\nDefine framerate in Hz.\nSize of free camera FFS.\nGain in dB.\nGain selector for parameter Gain allows to select …\nChromaticity gamma.\nLuminosity gamma.\nGet general purpose level.\nSet general purpose input mode.\nSelects general purpose input.\nSet general purpose output mode.\nSelects general purpose output.\nEnable High Dynamic Range feature.\nThe number of kneepoints in the PWLR.\nPosition of first kneepoint(in % of XI_PRM_EXPOSURE).\nPosition of second kneepoint (in % of XI_PRM_EXPOSURE).\nHeight of the Image provided by the device (in pixels).\nCamera housing back side temperature.\nCamera housing temperature.\nReturns hardware revision number.\nLast image black level counts. Can be used for Offline …\nbitdepth of data returned by function xiGetImage.\nOutput data format.\nThe alpha channel of RGB32 output image format.\nReturns 1 for color cameras.\nBuffer size in bytes sufficient for output image returned …\nReturns 1 for cameras that support cooling.\nReturns 1 if camera connected and works properly.\nValue of first kneepoint (% of sensor saturation).\nValue of second kneepoint (% of sensor saturation).\nDefine camera signalling LED functionality.\nSelects camera signalling LED.\nCurrent lens aperture value in stops. Examples: 2.8, 4, …\nAllows access to lens feature value currently selected by …\nSelects the current feature which is accessible by …\nLens focal distance in mm.\nLens focus distance in cm.\nMoves lens focus motor by steps set in …\nLens current focus movement value to be used by …\nStatus of lens control interface. This shall be set to …\nSet/get bandwidth(datarate)(in Megabits).\nActivates LUT.\nControl the index (offset) of the coefficient to access in …\nValue at entry LUTIndex of the LUT.\nCalculates White Balance(must be called during …\nHorizontal offset from the origin to the area of interest …\nVertical offset from the origin to the area of interest …\nDevice output data bit depth.\nDevice output data packing (or grouping) enabled. Packing …\nData packing type. Some cameras supports only specific …\nGetImage returns most recent frame.\nActivates/deactivates Region selected by Region Selector.\nSelects Region in Multiple ROI which parameters are set by …\nCorrection of row FPN.\nCamera sensor board temperature.\nSensor clock frequency in Hz.\nSensor clock frequency index. Sensor with selected …\nSensor output data bit depth.\nSelects the current feature which is accessible by …\nAllows access to sensor feature value currently selected …\nCurrent sensor mode. Allows to select sensor mode by one …\nNumber of output channels from sensor used for data …\nNumber of taps.\nSharpness Strength.\nChange sensor shutter type(CMOS sensor).\nSet sensor target temperature for cooling.\nSelects which test pattern type is generated by the …\nSelects which test pattern generator is controlled by the …\nImage capture timeout in milliseconds.\nCurrent format of pixels on transport layer.\nSpecifies the delay in microseconds (us) to apply after …\nSelects the type of trigger.\nGenerates an internal trigger. PRM_TRG_SOURCE must be set …\nDefines source of trigger.\nDefines how time stamp reset engine will be armed.\nDefines which source will be used for timestamp reset. …\nSize of used camera FFS.\nWhite balance blue coefficient.\nWhite balance green coefficient.\nWhite balance red coefficient.\nWidth of the Image provided by the device (in pixels).\nPvAPI, Prosilica GigE SDK\nPvAPI, Prosilica GigE SDK\n2 out of 16 decimation\n2 out of 4 decimation\n2 out of 8 decimation\nOff\nFixedRate\nFreerun\nSoftware\nSyncIn1\nSyncIn2\nBayer16\nBayer8\nBgr24\nBgra32\nMono16\nMono8\nRgb24\nRgba32\nQuickTime (obsolete, removed)\nQuickTime (obsolete, removed)\nSynonym for CAP_INTELPERC\nuEye Camera API\nuEye Camera API\nUnicap drivers (obsolete, removed)\nUnicap drivers (obsolete, removed)\nV4L/V4L2 capturing support\nSame as CAP_V4L\nVideo For Windows (obsolete, removed)\nVideo For Windows (obsolete, removed)\nMicrosoft Windows Runtime using Media Foundation\nMicrosoft Windows Runtime using Media Foundation\nXIMEA Camera API\nXIMEA Camera API\nXINE engine (Linux)\nXINE engine (Linux)\nDefaults to CV_8U.\nDefaults to CV_8U.\n(Read-only): Size of just encoded video frame. Note that …\n(Read-only): Size of just encoded video frame. Note that …\n(<strong>open-only</strong>) Hardware acceleration type (see …\n(<strong>open-only</strong>) Hardware acceleration type (see …\n(<strong>open-only</strong>) If non-zero, create new OpenCL context and …\n(<strong>open-only</strong>) If non-zero, create new OpenCL context and …\n(<strong>open-only</strong>) Hardware device index (select GPU if multiple …\n(<strong>open-only</strong>) Hardware device index (select GPU if multiple …\nIf it is not zero, the encoder will expect and encode …\nIf it is not zero, the encoder will expect and encode …\nNumber of stripes for parallel encoding. -1 for auto …\nNumber of stripes for parallel encoding. -1 for auto …\nCurrent quality (0..100%) of the encoded videostream. Can …\nCurrent quality (0..100%) of the encoded videostream. Can …\nPrefer to use H/W acceleration. If no one supported, then …\nPrefer to use H/W acceleration. If no one supported, then …\nDirectX 11\nDirectX 11\nlibmfx (Intel MediaSDK/oneVPL)\nlibmfx (Intel MediaSDK/oneVPL)\nDo not require any specific H/W acceleration, prefer …\nDo not require any specific H/W acceleration, prefer …\nVAAPI\nVAAPI\nVideo Acceleration type\nClass for video capturing from video files, image …\ncv::VideoCapture API backends identifier.\ncv::VideoCapture generic properties identifier.\nMutable methods for crate::videoio::VideoCapture\nConstant methods for crate::videoio::VideoCapture\nVideo writer class.\ncv::VideoWriter generic properties identifier.\nMutable methods for crate::videoio::VideoWriter\nConstant methods for crate::videoio::VideoWriter\nReturns backend API name or “UnknownVideoAPI(xxx)”\nReturns list of all available backends\nReturns description and ABI/API version of videoio plugin…\nReturns list of available backends which works via …\nReturns description and ABI/API version of videoio plugin…\nReturns list of available backends which works via …\nReturns description and ABI/API version of videoio plugin…\nReturns list of available backends which works via …\nReturns true if backend is available\nReturns true if backend is built in (false if backend is …\nMutable methods for crate::videoio::VideoCapture\nConstant methods for crate::videoio::VideoCapture\nMutable methods for crate::videoio::VideoWriter\nConstant methods for crate::videoio::VideoWriter")